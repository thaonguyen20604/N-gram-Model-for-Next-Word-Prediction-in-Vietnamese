{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### **Download library**"
      ],
      "metadata": {
        "id": "-KGWXyEoLahf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otUAB708Lji8",
        "outputId": "51434fb3-3981-4754-f7b1-3f4be75e64e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from pyvi) (1.6.1)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pyvi) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pyvi) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pyvi) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pyvi) (3.6.0)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->pyvi)\n",
            "  Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.12/dist-packages (from sklearn-crfsuite->pyvi) (4.67.1)\n",
            "Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.11 pyvi-0.1.1 sklearn-crfsuite-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Import Library**"
      ],
      "metadata": {
        "id": "rCVNs17dLLpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "import random\n",
        "from pyvi import ViTokenizer\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import math\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOs7mfwwLLCQ",
        "outputId": "046dfa35-80a6-428a-e230-fc12a123f009"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Crawl Data**"
      ],
      "metadata": {
        "id": "LP-GU6YeObPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crawl_tuoitre_rss(rss_url=\"https://tuoitre.vn/rss/thoi-su.rss\", \\\n",
        "                      output_file=\"tuoitre.txt\"):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    all_texts = []\n",
        "\n",
        "    # Load RSS feed (XML)\n",
        "    r = requests.get(rss_url, headers=headers)\n",
        "    soup = BeautifulSoup(r.text, \"xml\")\n",
        "\n",
        "    # Get link from tag <item><link>\n",
        "    links = [item.find(\"link\").text for item in soup.find_all(\"item\")]\n",
        "    print(\"Found\", len(links), \"paper from RSS\")\n",
        "\n",
        "    # Get content in each link\n",
        "    for link in links:\n",
        "        try:\n",
        "            art = requests.get(link, headers=headers)\n",
        "            soup_art = BeautifulSoup(art.text, \"html.parser\")\n",
        "\n",
        "            # Get content\n",
        "            paragraphs = soup_art.select(\"div.detail-content p\")\n",
        "            # print(paragraphs)\n",
        "            content = \" \".join(p.get_text() for p in paragraphs)\n",
        "            # print(content)\n",
        "\n",
        "            if content.strip():\n",
        "                all_texts.append(content)\n",
        "        except Exception as e:\n",
        "            print(\"Error:\", e)\n",
        "\n",
        "    # Save into .txt\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in all_texts:\n",
        "            f.write(line + \"\\n\")\n",
        "\n",
        "    print(f\"Crawl {len(all_texts)}, saved at {output_file}\")"
      ],
      "metadata": {
        "id": "RfPNY5jPOgF1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crawl_tuoitre_rss(\"https://tuoitre.vn/rss/thoi-su.rss\", \"tuoitre.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNGY761qO4Oe",
        "outputId": "d66c7ed0-3be3-4013-f8dc-44dcb1c69a27"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 50 paper from RSS\n",
            "Crawl 50, saved at tuoitre.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Data Preprocessing**"
      ],
      "metadata": {
        "id": "0kP8JwbOUpf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^0-9a-zA-ZÀ-Ỹà-ỹ\\s\\.,!?]\", \" \", text)\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "OEFNN2i1UtvE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"tuoitre.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "cleaned_text = clean_text(raw_text)\n",
        "\n",
        "with open(\"tuoitre_clean.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(cleaned_text)"
      ],
      "metadata": {
        "id": "2Vc77SYQVcL1"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Model**"
      ],
      "metadata": {
        "id": "ciaGZhKBMzbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Defination"
      ],
      "metadata": {
        "id": "jwW5_s89a4Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NgramModel:\n",
        "    def __init__(self, n, alpha=0.01):\n",
        "        self.n = n\n",
        "        self.ngram_counts = Counter()\n",
        "        self.context_counts = Counter()\n",
        "        self.vocab = set()\n",
        "        self.alpha = alpha\n",
        "\n",
        "\n",
        "    '''\n",
        "    Tokenize text function\n",
        "    '''\n",
        "    def tokenize_text(self, text):\n",
        "        # separate sentences\n",
        "        sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
        "        # tokenize\n",
        "        tokens = [ViTokenizer.tokenize(sent).split() for sent in sentences]\n",
        "        # Add start-of-sentence (<s>) and end-of-sentence (</s>) tokens\n",
        "        # to each sentence for proper n-gram training\n",
        "        tokens = [[\"<s>\"] + t + [\"</s>\"] for t in tokens]\n",
        "        return tokens\n",
        "\n",
        "\n",
        "    '''\n",
        "    Tokenize sentence function\n",
        "    '''\n",
        "    def tokenize_sentence(self, sentence):\n",
        "        return [\"<s>\"] + ViTokenizer.tokenize(sentence.lower()).split() + [\"</s>\"]\n",
        "\n",
        "\n",
        "    '''\n",
        "    Model training function\n",
        "    '''\n",
        "    def train(self, file_path):\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read().lower()\n",
        "\n",
        "        tokens = self.tokenize_text(text)\n",
        "\n",
        "        for sent in tokens:\n",
        "            self.vocab.update(sent)\n",
        "            for i in range(len(sent)-self.n+1):\n",
        "                ngram = tuple(sent[i:i+self.n])\n",
        "                context = ngram[:-1]\n",
        "                self.ngram_counts[ngram] += 1\n",
        "                self.context_counts[context] += 1\n",
        "\n",
        "\n",
        "    '''\n",
        "    Probability n-gram\n",
        "    '''\n",
        "    def prob(self, ngram):\n",
        "        context = ngram[:-1]\n",
        "        count_ngram = self.ngram_counts[ngram]\n",
        "        count_context = self.context_counts[context]\n",
        "        V = len(self.vocab)\n",
        "        return (count_ngram + self.alpha) / (count_context + V * self.alpha)\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "    Predict the next word\n",
        "    '''\n",
        "    def predict_next(self, context, top_k=5):\n",
        "        if isinstance(context, str):\n",
        "            context = ViTokenizer.tokenize(context.lower()).split()\n",
        "        context = tuple(context[-(self.n-1):])\n",
        "        candidates = {w: self.prob(context + (w,)) for w in self.vocab}\n",
        "        sorted_candidates = sorted(candidates.items(),\n",
        "                                   key=lambda x: x[1], reverse=True)\n",
        "        return sorted_candidates[:top_k]\n",
        "\n",
        "\n",
        "    '''\n",
        "    Probability of a sentence\n",
        "    '''\n",
        "    def sentence_prob(self, sentence):\n",
        "        tokens = self.tokenize_sentence(sentence)\n",
        "        log_prob = 0.0\n",
        "        for i in range(len(tokens)-self.n+1):\n",
        "            ngram = tuple(tokens[i:i+self.n])\n",
        "            log_prob += math.log(self.prob(ngram))\n",
        "        return math.exp(log_prob)\n",
        "\n"
      ],
      "metadata": {
        "id": "qQaJW10nphyP"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Training"
      ],
      "metadata": {
        "id": "451It02ja7Xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train bigram model\n",
        "lm2 = NgramModel(n=2)\n",
        "lm2.train(\"tuoitre_clean.txt\")\n",
        "\n",
        "# Train trigram model\n",
        "lm3 = NgramModel(n=3)\n",
        "lm3.train(\"tuoitre_clean.txt\")\n",
        "\n",
        "# Test probability of one sentence\n",
        "sentence = \"nhiều khu trung tâm thương mại\"\n",
        "print(\"Prob (bigram model):\", lm2.sentence_prob(sentence))\n",
        "print(\"Prob (trigram model):\", lm3.sentence_prob(sentence))\n",
        "\n",
        "# Predict the next word\n",
        "print(\"\\nPrediction (bigram model):\", lm2.predict_next(sentence))\n",
        "print(\"Prediction (trigram model):\", lm3.predict_next(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y6LlDfGPdam",
        "outputId": "874805ad-5c91-4f2b-a8a5-4af11c6c6801"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prob (bigram model): 3.763653436918376e-12\n",
            "Prob (trigram model): 6.332875283257362e-09\n",
            "\n",
            "Prediction (bigram model): [(',', 0.05068078668683811), ('xây_dựng', 0.025466464952092788), ('điện_tử', 0.025466464952092788), ('sản_phẩm', 0.025466464952092788), ('vincom', 0.025466464952092788)]\n",
            "Prediction (trigram model): [('vincom', 0.03000594177064765), ('quyết_định', 0.00029708853238265), ('chủ_đề', 0.00029708853238265), ('dỡ', 0.00029708853238265), ('cai_trị', 0.00029708853238265)]\n"
          ]
        }
      ]
    }
  ]
}